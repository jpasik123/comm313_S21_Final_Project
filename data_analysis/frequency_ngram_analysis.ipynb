{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Analyses, N-gram Lists, Word Comparisons\n",
    "\n",
    "### In this notebook, you will find:\n",
    "- Loaded corpora from JSON files of various song dictionaries \n",
    "- Detailed text analysis of lyrics, separated by section headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Commjhub/jupyterhub/comm\n",
      "[nltk_data]     318_fall2019/jpasik123/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Additional modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lyricsgenius\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk import Text\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "sect_stoppers = ['pre-chorus','refrain','chorus','verse','intro','outro','bridge','verse 1','verse 2','verse 3','verse 4','1','2','3','4','Tim McGraw','Faith Hill','Tim McGraw & Faith Hill']\n",
    "for x in sect_stoppers:\n",
    "    stop_words.append(x)\n",
    "# pos tagging\n",
    "from nltk import pos_tag, pos_tag_sents, FreqDist, ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_strip = '.,!][?;$\"-()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_charts = json.load(open('../data/charts/all_charts.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Decade', 'Title', 'Artist', 'Gender', 'lyrics', 'tokens'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_charts['all_90s'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'pre-chorus',\n",
       " 'refrain',\n",
       " 'chorus',\n",
       " 'verse',\n",
       " 'intro',\n",
       " 'outro',\n",
       " 'bridge',\n",
       " 'verse 1',\n",
       " 'verse 2',\n",
       " 'verse 3',\n",
       " 'verse 4',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " 'Tim McGraw',\n",
       " 'Faith Hill',\n",
       " 'Tim McGraw & Faith Hill']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word and Song Frequencies; N-gram lists (bigrams and trigrams) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All 1990s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 words in your `all_90s` corpus\n",
      "==================================\n",
      "[('love', 108), (\"i'm\", 75), ('know', 74), ('oh', 70), ('yeah', 56), ('like', 55), ('got', 50), ('get', 47), ('let', 47), ('little', 45), ('wanna', 44), (\"ain't\", 42), ('one', 38), ('never', 38), ('way', 37), ('tell', 36), ('girl', 35), (\"i've\", 35), ('take', 34), ('ya', 34), ('come', 33), ('say', 33), ('go', 33), ('boy', 32), ('said', 32), ('make', 32), ('baby', 30), ('heart', 30), ('away', 29), ('gonna', 28), ('think', 28), ('well', 27), ('right', 27), ('maria', 27), ('time', 26), ('man', 25), ('ever', 25), ('feel', 25), ('want', 24), ('night', 22), ('would', 22), ('knows', 22), ('world', 21), ('hey', 21), (\"can't\", 21), ('kiss', 21), ('maybe', 19), ('really', 19), ('passionate', 19), ('kisses', 19)]\n",
      "Top 50 # of songs each type occurs in your `all_90s` corpus\n",
      "==================================\n",
      "[('know', 22), (\"i'm\", 21), ('like', 20), ('love', 19), ('night', 19), ('oh', 19), ('got', 18), ('one', 18), ('said', 17), ('never', 17), ('get', 16), (\"ain't\", 15), ('right', 15), (\"i've\", 15), ('come', 14), ('man', 14), ('let', 14), ('time', 14), ('go', 14), ('take', 14), ('long', 13), ('way', 13), ('baby', 13), ('gonna', 12), ('away', 12), ('yeah', 12), ('back', 11), ('say', 11), ('little', 11), ('heart', 11), ('eyes', 10), ('world', 10), ('make', 10), ('girl', 10), ('boy', 9), ('last', 9), ('every', 9), ('good', 9), ('tell', 9), ('day', 9), (\"can't\", 9), ('says', 8), ('want', 8), ('took', 8), ('life', 8), ('see', 8), (\"there's\", 8), ('well', 8), ('ever', 8), ('around', 8)]\n",
      "Top 50 bigrams in your `all_90s` corpus\n",
      "==================================\n",
      "[('passionate kisses', 19), ('little goodbyes', 16), ('ay yeah', 16), (\"i'm hurry\", 14), ('love start', 14), (\"start slippin'\", 14), ('would ya', 14), ('love boy', 13), ('kisses passionate', 13), ('know beautiful', 13), ('way love', 12), (\"i've got\", 12), ('maria oh', 12), ('oh marie', 12), ('marie love', 12), ('love girl', 12), ('walkaway joe', 11), ('yeah yeah', 9), ('kiss kiss', 9), ('got friends', 9), ('friends low', 9), ('low places', 9), ('tim mcgraw', 9), ('yeah ay', 9), ('boy love', 8), (\"here's one\", 8), ('one chance', 8), ('chance fancy', 8), ('fancy let', 8), ('hey hey', 8), ('really gotta', 8), ('tell heart', 8), ('heart achy', 8), ('achy breaky', 8), ('breaky heart', 8), ('like love', 8), ('say wanna', 8), ('wanna get', 8), ('live without', 7), ('want know', 7), ('feel like', 7), ('watermelon crawl', 7), ('try think', 7), ('chasing neon', 7), ('neon rainbow', 7), ('living honkytonk', 7), ('honkytonk dream', 7), ('hurry get', 7), ('get things', 7), ('things done', 7)]\n",
      "Top 50 trigrams in your `all_90s` corpus\n",
      "==================================\n",
      "[(\"love start slippin'\", 14), ('passionate kisses passionate', 13), ('kisses passionate kisses', 13), ('maria oh marie', 12), ('oh marie love', 12), ('marie love girl', 12), ('got friends low', 9), ('friends low places', 9), ('ay yeah ay', 9), ('yeah ay yeah', 9), ('love boy love', 8), ('boy love boy', 8), (\"here's one chance\", 8), ('one chance fancy', 8), ('chance fancy let', 8), ('tell heart achy', 8), ('heart achy breaky', 8), ('achy breaky heart', 8), ('chasing neon rainbow', 7), ('living honkytonk dream', 7), (\"i'm hurry get\", 7), ('hurry get things', 7), ('get things done', 7), (\"rush rush life's\", 7), (\"rush life's fun\", 7), (\"life's fun really\", 7), ('fun really gotta', 7), ('really gotta live', 7), ('gotta live die', 7), (\"live die i'm\", 7), (\"die i'm hurry\", 7), (\"i'm hurry know\", 7), ('let love start', 7), (\"start slippin' love\", 7), (\"slippin' love start\", 7), (\"start slippin' away\", 7), ('could ya would', 7), ('ya would ya', 7), (\"would ya ain't\", 7), (\"ya ain't ya\", 7), (\"ain't ya gonna\", 7), ('ya gonna asked', 7), ('gonna asked would', 7), ('asked would ya', 7), ('would ya wanna', 7), ('ya wanna baby', 7), ('wanna baby tonight', 7), ('tim mcgraw &', 7), ('mcgraw & faith', 7), ('& faith hill', 7)]\n"
     ]
    }
   ],
   "source": [
    "word_freq_90s = Counter() ##tally of # of times each type occurs in TOTAL \n",
    "song_freq_90s = Counter() ## tally of # of songs each type occurs in (each type only counted once for each song it occurs in)\n",
    "bigrams_90s_dist = Counter()\n",
    "trigrams_90s_dist = Counter()\n",
    "\n",
    "# 1. loop over each song in chart\n",
    "for song in all_charts['all_90s']:\n",
    "    raw_lyrics = song['lyrics']\n",
    "    song_toks = []\n",
    "    temp = tokenize(raw_lyrics, lowercase=True,strip_chars=char_to_strip)\n",
    "    for tok in temp:\n",
    "        if tok not in stop_words:\n",
    "            song_toks.append(tok)\n",
    "    word_freq_90s.update(song_toks)\n",
    "    unique_type = set(song_toks) \n",
    "    song_freq_90s.update(unique_type) # update song_freq_90s with the types (i.e. unique values in tokens)\n",
    "    bigram_90s = get_ngram_tokens(song_toks, n=2)\n",
    "    bigrams_90s_dist.update(bigram_90s)\n",
    "    trigrams_90s = get_ngram_tokens(song_toks, n=3)\n",
    "    trigrams_90s_dist.update(trigrams_90s)\n",
    "\n",
    "print('Top 50 words in your `all_90s` corpus\\n', '='*34, sep='')\n",
    "print(word_freq_90s.most_common(50))\n",
    "\n",
    "print('Top 50 # of songs each type occurs in your `all_90s` corpus\\n', '='*34, sep='')\n",
    "print(song_freq_90s.most_common(50))\n",
    "\n",
    "print('Top 50 bigrams in your `all_90s` corpus\\n', '='*34, sep='')\n",
    "print(bigrams_90s_dist.most_common(50))\n",
    "\n",
    "print('Top 50 trigrams in your `all_90s` corpus\\n', '='*34, sep='')\n",
    "print(trigrams_90s_dist.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Observations \n",
    "\n",
    "As can be seen in the printed results above, a majority of the most frequently recurring tokens are filler words such as \"oh\", \"yeah\", and \"really\". However, there are some substantive words that stand out and will be used in later, more detailed analysis. The words that stand out the most to me are: \"love\", \"little\", \"girl\", \"boy\", \"man\", \"world\", \"passionate\", and \"kiss(es)\". \n",
    "\n",
    "Additionally, when looking at these frequencies, it is important to note that the repetition of certain words (and thus higher frequency counts) may be skewed if that specific word is repeated in say, the choruses or bridges of a song. That being said, it is helpful to pay close attention to the `song_freq` results because those reveal the number of songs each type occurs in (each type only counted once for each song it occurs in).\n",
    "\n",
    "When looking at the bigrams and trigrams, it seems to be that many are word pairs and combinations coming from the same song (i.e. \"achy breaky\" and \"breaky heart\" and \"got friends low\" and \"friends low places\"). It will be interesting to further analyze some of the tokens and n-grams throughout this project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average type-token ratio among `all_90s` songs is 40.612394807300554\n"
     ]
    }
   ],
   "source": [
    "## Calculating type-token ratio for 90s\n",
    "# Finding amount of unique words across corpora\n",
    "\n",
    "ttr_90s = []\n",
    "\n",
    "for song in all_charts['all_90s']:\n",
    "    toks_90s = song['tokens']\n",
    "    ttr = len(set(toks_90s)) / len(toks_90s) * 100\n",
    "    ttr_90s.append(ttr)\n",
    "\n",
    "print('the average type-token ratio among `all_90s` songs is {}'.format(sum(ttr_90s) / len(ttr_90s)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All 2010s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 words in your `all_2010s` corpus\n",
      "==================================\n",
      "[('like', 108), (\"i'm\", 97), ('yeah', 87), ('back', 80), ('little', 62), ('get', 60), ('got', 58), (\"ain't\", 53), ('baby', 52), ('every', 51), ('know', 46), ('never', 45), ('gonna', 43), ('oh', 42), ('take', 40), (\"'em\", 39), ('make', 38), ('right', 36), ('one', 36), ('oooh', 36), ('see', 35), ('go', 35), ('road', 34), ('good', 33), ('need', 33), ('hope', 33), ('think', 32), ('way', 31), ('rock', 31), ('away', 30), ('ever', 29), ('mama', 29), ('around', 28), ('hey', 28), ('heart', 27), ('name', 27), ('wanna', 27), ('could', 26), ('changed', 25), (\"'cause\", 24), ('night', 24), (\"i'll\", 23), ('thing', 22), ('dirt', 22), ('girl', 22), ('song', 22), ('free', 22), ('man', 21), ('come', 21), (\"can't\", 21)]\n",
      "Top 50 # of songs each type occurs in your `all_2010s` corpus\n",
      "==================================\n",
      "[('like', 23), ('back', 22), ('get', 22), ('yeah', 21), (\"i'm\", 19), ('know', 18), ('go', 18), ('got', 17), (\"ain't\", 17), ('take', 15), ('one', 15), ('good', 14), ('right', 14), ('baby', 14), ('little', 14), ('see', 13), ('time', 13), ('oh', 13), ('way', 13), ('heart', 12), ('never', 12), ('around', 12), ('make', 12), ('every', 11), ('look', 11), (\"i'll\", 11), ('night', 11), ('come', 11), ('gonna', 10), ('place', 10), (\"'cause\", 10), ('put', 10), ('need', 10), (\"can't\", 10), ('think', 10), ('feel', 10), ('ever', 9), ('old', 9), ('turn', 9), ('say', 9), ('love', 9), ('day', 9), ('road', 9), ('sun', 9), ('well', 9), ('smile', 8), ('said', 8), ('thought', 8), ('man', 8), ('made', 8)]\n",
      "Top 50 bigrams in your `all_2010s` corpus\n",
      "==================================\n",
      "[('oooh oooh', 34), ('blown away', 19), ('got name', 19), ('name changed', 18), ('changed back', 18), ('every little', 18), ('rock mama', 18), ('yeah yeah', 15), ('little thing', 15), ('take road', 14), ('road less', 14), ('less traveled', 14), ('whiskey glasses', 13), ('afraid take', 12), ('back yeah', 12), ('little bit', 12), ('make wanna', 12), ('mama like', 12), (\"like i'm\", 11), ('little dirt', 11), ('dirt boots', 11), (\"god's country\", 11), ('goes like', 11), ('taste tequila', 11), ('roll windows', 10), ('got little', 10), ('always stay', 10), ('stay humble', 10), ('humble kind', 10), ('mama rock', 10), ('broken halos', 10), (\"let 'em\", 9), ('back road', 9), ('bit stronger', 9), (\"i'm getting\", 9), (\"i'm done\", 9), ('windows cruise', 9), (\"line 'em\", 9), (\"knock 'em\", 9), (\"i'ma need\", 9), ('drunk plane', 9), ('never gonna', 8), ('oh oohwhoa', 8), ('something water', 8), (\"i'm gonna\", 8), ('might little', 8), ('baby song', 8), ('song make', 8), ('wanna roll', 8), ('like back', 8)]\n",
      "Top 50 trigrams in your `all_2010s` corpus\n",
      "==================================\n",
      "[('oooh oooh oooh', 32), ('got name changed', 18), ('name changed back', 18), ('take road less', 14), ('road less traveled', 14), ('every little thing', 14), ('changed back yeah', 12), ('back yeah yeah', 12), ('rock mama like', 12), ('little dirt boots', 11), ('always stay humble', 10), ('stay humble kind', 10), ('afraid take road', 9), ('little bit stronger', 9), ('baby song make', 8), ('song make wanna', 8), ('make wanna roll', 8), ('wanna roll windows', 8), ('roll windows cruise', 8), (\"knock 'em back\", 8), ('hey mama rock', 8), ('blown away blown', 7), ('away blown away', 7), ('see world whiskey', 7), ('world whiskey glasses', 7), (\"i'm getting drunk\", 7), ('getting drunk plane', 7), ('broken halos used', 7), ('halos used shine', 7), ('yeah guess that’s', 6), ('guess that’s church', 6), ('follow arrow wherever', 6), ('arrow wherever points', 6), ('girl country song', 6), ('must something water', 6), ('hope cheats like', 6), ('remember every little', 6), ('got little dirt', 6), (\"free free we'll\", 6), (\"free we'll ever\", 6), ('woahohohoh woahohohoh woahohohohohohoh', 6), (\"i'ma need whiskey\", 6), ('need whiskey glasses', 6), (\"line 'em line\", 6), (\"'em line 'em\", 6), (\"'em back knock\", 6), (\"back knock 'em\", 6), (\"fill 'em fill\", 6), (\"'em fill 'em\", 6), ('never gonna man', 5)]\n"
     ]
    }
   ],
   "source": [
    "word_freq_2010s = Counter() ##tally of # of times each type occurs in TOTAL \n",
    "song_freq_2010s = Counter() ## tally of # of songs each type occurs in (each type only counted once for each song it occurs in)\n",
    "bigrams_2010s_dist = Counter()\n",
    "trigrams_2010s_dist = Counter()\n",
    "\n",
    "# 1. loop over each song in chart\n",
    "for song in all_charts['all_2010s']:\n",
    "    raw_lyrics = song['lyrics']\n",
    "    song_toks = []\n",
    "    temp = tokenize(raw_lyrics, lowercase=True,strip_chars=char_to_strip)\n",
    "    for tok in temp:\n",
    "        if tok not in stop_words:\n",
    "            song_toks.append(tok)\n",
    "    word_freq_2010s.update(song_toks)\n",
    "    unique_type = set(song_toks) \n",
    "    song_freq_2010s.update(unique_type) \n",
    "    bigram_2010s = get_ngram_tokens(song_toks, n=2)\n",
    "    bigrams_2010s_dist.update(bigram_2010s)\n",
    "    trigrams_2010s = get_ngram_tokens(song_toks, n=3)\n",
    "    trigrams_2010s_dist.update(trigrams_2010s)\n",
    "\n",
    "print('Top 50 words in your `all_2010s` corpus\\n', '='*34, sep='')\n",
    "print(word_freq_2010s.most_common(50))\n",
    "\n",
    "print('Top 50 # of songs each type occurs in your `all_2010s` corpus\\n', '='*34, sep='')\n",
    "print(song_freq_2010s.most_common(50))\n",
    "\n",
    "print('Top 50 bigrams in your `all_2010s` corpus\\n', '='*34, sep='')\n",
    "print(bigrams_2010s_dist.most_common(50))\n",
    "\n",
    "print('Top 50 trigrams in your `all_2010s` corpus\\n', '='*34, sep='')\n",
    "print(trigrams_2010s_dist.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "With the `all_2010s` data, the words \"little\", \"baby\", \"road\", \"mama\", \"name\", \"dirt\", \"man\", \"girl\" and \"hope\" occurred most frequently. In addition, looking over the bigrams list, there \n",
    "are some distinct pairs such as: \"whiskey glasses\", \"dirt boots\", \"taste tequila\", \"back road\", \"drunk plane\". Lastly, being more familiar with these songs from the 2010s myself, I recognize that most of the trigrams reference either the titles of the songs or main words in the choruses that make it easy to determine which songs the words are coming from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average type-token ratio among 2010s songs is 37.27572078056541\n"
     ]
    }
   ],
   "source": [
    "## Calculating type-token ratio for 2010s\n",
    "# Finding amount of unique words across corpora\n",
    "\n",
    "\n",
    "ttr_2010s = []\n",
    "\n",
    "for song in all_charts['all_2010s']:\n",
    "    toks_2010s = song['tokens']\n",
    "    ttr_10s = len(set(toks_2010s)) / len(toks_2010s) * 100\n",
    "    ttr_2010s.append(ttr_10s)\n",
    "\n",
    "print('the average type-token ratio among 2010s songs is {}'.format(sum(ttr_2010s) / len(ttr_2010s)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 words in your `all_female` corpus\n",
      "==================================\n",
      "[('yeah', 83), ('like', 65), ('little', 64), ('oh', 61), (\"i'm\", 58), ('get', 52), ('got', 51), ('know', 50), ('love', 49), ('back', 49), ('every', 46), ('never', 44), ('let', 43), (\"ain't\", 40), ('way', 40), ('one', 39), ('away', 38), ('gonna', 38), ('ever', 37), ('take', 37), ('go', 35), ('hope', 35), ('boy', 34), ('said', 34), ('baby', 31), ('well', 27), ('kiss', 27), ('right', 26), ('make', 26), ('time', 25), ('thing', 25), ('feel', 25), ('could', 25), ('changed', 25), ('good', 24), ('prechorus', 24), ('heart', 24), ('say', 23), ('hey', 23), ('come', 22), ('name', 22), ('day', 22), ('put', 22), ('maybe', 22), ('knows', 22), (\"i've\", 22), ('think', 22), ('road', 21), ('man', 21), ('left', 21)]\n",
      "Top 50 # of songs each type occurs in your `all_female` corpus\n",
      "==================================\n",
      "[('like', 21), ('one', 20), ('get', 19), ('back', 19), ('oh', 18), ('got', 16), ('said', 16), (\"i'm\", 16), ('good', 16), ('know', 16), ('yeah', 16), (\"ain't\", 15), ('time', 15), ('take', 15), ('gonna', 14), ('little', 14), ('go', 14), ('come', 13), ('love', 13), ('baby', 13), ('heart', 13), ('right', 13), ('away', 12), ('way', 12), ('every', 12), ('around', 12), ('never', 12), ('man', 11), ('let', 11), ('say', 11), ('night', 11), ('prechorus', 11), ('ever', 11), ('put', 11), ('make', 11), ('look', 10), ('made', 10), ('world', 10), ('well', 10), ('day', 10), (\"i've\", 10), ('last', 9), ('town', 9), ('left', 9), ('life', 9), ('see', 9), ('tell', 9), (\"'cause\", 9), ('still', 8), ('boy', 8)]\n",
      "Top 50 bigrams in your `all_female` corpus\n",
      "==================================\n",
      "[('yeah yeah', 24), ('passionate kisses', 19), ('got name', 19), ('name changed', 18), ('changed back', 18), ('little goodbyes', 16), ('blown away', 16), ('little thing', 15), ('take road', 14), ('road less', 14), ('less traveled', 14), ('every little', 14), ('love boy', 13), ('kisses passionate', 13), ('way love', 12), ('little bit', 12), ('back yeah', 12), ('walkaway joe', 11), ('kiss kiss', 9), (\"let 'em\", 9), ('afraid take', 9), ('bit stronger', 9), (\"i'm done\", 9), ('boy love', 8), (\"here's one\", 8), ('one chance', 8), ('chance fancy', 8), ('fancy let', 8), ('get little', 8), ('best thing', 8), ('hey hey', 8), ('never gonna', 8), ('oh oohwhoa', 8), ('something water', 8), ('live without', 7), ('watermelon crawl', 7), ('try think', 7), ('away blown', 7), ('burning house', 7), (\"like i'm\", 7), ('miranda lambert', 7), ('black like', 7), ('independence day', 6), ('feel like', 6), (\"i've got\", 6), ('strawberry wine', 6), (\"oh i'm\", 6), ('heads carolina', 6), ('carolina tails', 6), ('tails california', 6)]\n",
      "Top 50 trigrams in your `all_female` corpus\n",
      "==================================\n",
      "[('got name changed', 18), ('name changed back', 18), ('take road less', 14), ('road less traveled', 14), ('every little thing', 14), ('passionate kisses passionate', 13), ('kisses passionate kisses', 13), ('changed back yeah', 12), ('back yeah yeah', 12), ('afraid take road', 9), ('little bit stronger', 9), ('love boy love', 8), ('boy love boy', 8), (\"here's one chance\", 8), ('one chance fancy', 8), ('chance fancy let', 8), ('blown away blown', 7), ('away blown away', 7), ('heads carolina tails', 6), ('carolina tails california', 6), ('yeah guess that’s', 6), ('guess that’s church', 6), ('follow arrow wherever', 6), ('arrow wherever points', 6), ('girl country song', 6), ('must something water', 6), ('hope cheats like', 6), ('remember every little', 6), (\"said here's one\", 5), ('feel way feel', 5), ('wide open spaces', 5), ('knows high stakes', 5), ('like strawberry wine', 5), ('feeling like centrifugal', 5), ('like centrifugal motion', 5), ('never gonna man', 5), (\"careless man's careful\", 5), (\"man's careful daughter\", 5), ('careful daughter best', 5), ('daughter best thing', 5), (\"best thing that's\", 5), (\"thing that's ever\", 5), (\"that's ever mine\", 5), ('less traveled wear', 5), ('traveled wear boots', 5), ('wear boots kick', 5), ('boots kick gravel', 5), ('kick gravel afraid', 5), ('gravel afraid take', 5), ('yeah yeah got', 5)]\n"
     ]
    }
   ],
   "source": [
    "f_word_freq = Counter() ##tally of # of times each type occurs in TOTAL \n",
    "f_song_freq = Counter() ## tally of # of songs each type occurs in (each type only counted once for each song it occurs in)\n",
    "f_bigrams_dist = Counter()\n",
    "f_trigrams_dist = Counter()\n",
    "\n",
    "# 1. loop over each song in chart\n",
    "for song in all_charts['all_female']:\n",
    "    raw_lyrics = song['lyrics']\n",
    "    song_toks = []\n",
    "    temp = tokenize(raw_lyrics, lowercase=True,strip_chars=char_to_strip)\n",
    "    for tok in temp:\n",
    "        if tok not in stop_words:\n",
    "            song_toks.append(tok)\n",
    "    f_word_freq.update(song_toks)\n",
    "    unique_type = set(song_toks) \n",
    "    f_song_freq.update(unique_type) \n",
    "    f_bigrams = get_ngram_tokens(song_toks, n=2)\n",
    "    f_bigrams_dist.update(f_bigrams)\n",
    "    f_trigrams = get_ngram_tokens(song_toks, n=3)\n",
    "    f_trigrams_dist.update(f_trigrams)\n",
    "\n",
    "print('Top 50 words in your `all_female` corpus\\n', '='*34, sep='')\n",
    "print(f_word_freq.most_common(50))\n",
    "\n",
    "print('Top 50 # of songs each type occurs in your `all_female` corpus\\n', '='*34, sep='')\n",
    "print(f_song_freq.most_common(50))\n",
    "\n",
    "print('Top 50 bigrams in your `all_female` corpus\\n', '='*34, sep='')\n",
    "print(f_bigrams_dist.most_common(50))\n",
    "\n",
    "print('Top 50 trigrams in your `all_female` corpus\\n', '='*34, sep='')\n",
    "print(f_trigrams_dist.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "Within the `all_female` charts, the words that stand out to me are: \"little\", \"love\", \"hope\", \"boy\", \"baby\", \"kiss\", \"heart\", and \"road\". Looking at these words out of context, one can infer that  country songs written by female artists tend to share narratives about love. It will be interesting to analyze their context further with KWIC concordance analyses.  \n",
    "\n",
    "Again, the bigrams and trigrams hint at the titles or main messages of the songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 words in your `all_male` corpus\n",
      "==================================\n",
      "[(\"i'm\", 114), ('like', 98), ('love', 78), ('know', 70), ('yeah', 60), ('got', 57), ('wanna', 57), ('get', 55), (\"ain't\", 55), ('oh', 51), ('baby', 51), ('ya', 48), ('back', 46), ('make', 44), ('little', 43), ('girl', 40), ('never', 39), ('think', 38), ('take', 37), ('right', 37), ('oooh', 36), ('one', 35), ('see', 34), ('heart', 33), ('go', 33), ('gonna', 33), ('come', 32), ('tell', 31), ('rock', 30), ('night', 29), (\"can't\", 29), (\"i've\", 28), ('way', 28), ('mama', 28), (\"'em\", 28), ('need', 28), ('maria', 27), ('hey', 26), ('say', 25), ('man', 25), ('around', 25), (\"i'll\", 24), ('whiskey', 23), ('world', 23), ('beautiful', 23), ('let', 22), ('away', 21), ('time', 21), ('good', 21), ('would', 20)]\n",
      "Top 50 # of songs each type occurs in your `all_male` corpus\n",
      "==================================\n",
      "[('know', 24), (\"i'm\", 24), ('like', 22), ('night', 19), ('get', 19), ('got', 19), ('go', 18), ('never', 17), ('yeah', 17), (\"ain't\", 17), ('right', 16), ('love', 15), ('oh', 14), ('take', 14), ('way', 14), ('back', 14), ('baby', 14), ('one', 13), (\"can't\", 13), (\"i've\", 13), ('time', 12), ('come', 12), ('see', 12), ('feel', 12), ('man', 11), ('long', 11), ('make', 11), ('girl', 11), ('little', 11), ('heart', 10), ('eyes', 10), ('think', 10), ('said', 9), ('say', 9), ('new', 9), ('old', 9), ('let', 9), (\"'cause\", 9), ('thought', 8), ('place', 8), (\"there's\", 8), ('world', 8), ('gonna', 8), ('sweet', 8), (\"i'll\", 8), ('tell', 8), ('around', 8), ('really', 8), ('hey', 8), ('day', 8)]\n",
      "Top 50 bigrams in your `all_male` corpus\n",
      "==================================\n",
      "[('oooh oooh', 34), ('rock mama', 18), ('ay yeah', 16), (\"i'm hurry\", 14), ('love start', 14), (\"start slippin'\", 14), ('would ya', 14), ('know beautiful', 13), ('whiskey glasses', 13), ('maria oh', 12), ('oh marie', 12), ('marie love', 12), ('love girl', 12), ('make wanna', 12), ('mama like', 12), ('got little', 11), ('little dirt', 11), ('dirt boots', 11), (\"god's country\", 11), ('goes like', 11), ('taste tequila', 11), ('always stay', 10), ('stay humble', 10), ('humble kind', 10), ('mama rock', 10), ('broken halos', 10), ('got friends', 9), ('friends low', 9), ('low places', 9), (\"i'm gonna\", 9), ('tim mcgraw', 9), ('yeah ay', 9), ('roll windows', 9), ('windows cruise', 9), (\"line 'em\", 9), (\"knock 'em\", 9), (\"i'ma need\", 9), ('drunk plane', 9), (\"i've got\", 8), ('tell heart', 8), ('heart achy', 8), ('achy breaky', 8), ('breaky heart', 8), ('like love', 8), ('baby tonight', 8), ('say wanna', 8), ('wanna get', 8), ('might little', 8), ('baby song', 8), ('song make', 8)]\n",
      "Top 50 trigrams in your `all_male` corpus\n",
      "==================================\n",
      "[('oooh oooh oooh', 32), (\"love start slippin'\", 14), ('maria oh marie', 12), ('oh marie love', 12), ('marie love girl', 12), ('rock mama like', 12), ('little dirt boots', 11), ('always stay humble', 10), ('stay humble kind', 10), ('got friends low', 9), ('friends low places', 9), ('ay yeah ay', 9), ('yeah ay yeah', 9), ('tell heart achy', 8), ('heart achy breaky', 8), ('achy breaky heart', 8), ('baby song make', 8), ('song make wanna', 8), ('make wanna roll', 8), ('wanna roll windows', 8), ('roll windows cruise', 8), (\"knock 'em back\", 8), ('hey mama rock', 8), ('chasing neon rainbow', 7), ('living honkytonk dream', 7), (\"i'm hurry get\", 7), ('hurry get things', 7), ('get things done', 7), (\"rush rush life's\", 7), (\"rush life's fun\", 7), (\"life's fun really\", 7), ('fun really gotta', 7), ('really gotta live', 7), ('gotta live die', 7), (\"live die i'm\", 7), (\"die i'm hurry\", 7), (\"i'm hurry know\", 7), ('let love start', 7), (\"start slippin' love\", 7), (\"slippin' love start\", 7), (\"start slippin' away\", 7), ('could ya would', 7), ('ya would ya', 7), (\"would ya ain't\", 7), (\"ya ain't ya\", 7), (\"ain't ya gonna\", 7), ('ya gonna asked', 7), ('gonna asked would', 7), ('asked would ya', 7), ('would ya wanna', 7)]\n"
     ]
    }
   ],
   "source": [
    "m_word_freq = Counter() ##tally of # of times each type occurs in TOTAL \n",
    "m_song_freq = Counter() ## tally of # of songs each type occurs in (each type only counted once for each song it occurs in)\n",
    "m_bigrams_dist = Counter()\n",
    "m_trigrams_dist = Counter()\n",
    "\n",
    "# 1. loop over each song in chart\n",
    "for song in all_charts['all_male']:\n",
    "    raw_lyrics = song['lyrics']\n",
    "    song_toks = []\n",
    "    temp = tokenize(raw_lyrics, lowercase=True,strip_chars=char_to_strip)\n",
    "    for tok in temp:\n",
    "        if tok not in stop_words:\n",
    "            song_toks.append(tok)\n",
    "    m_word_freq.update(song_toks)\n",
    "    unique_type = set(song_toks) \n",
    "    m_song_freq.update(unique_type) \n",
    "    m_bigrams = get_ngram_tokens(song_toks, n=2)\n",
    "    m_bigrams_dist.update(m_bigrams)\n",
    "    m_trigrams = get_ngram_tokens(song_toks, n=3)\n",
    "    m_trigrams_dist.update(m_trigrams)\n",
    "\n",
    "print('Top 50 words in your `all_male` corpus\\n', '='*34, sep='')\n",
    "print(m_word_freq.most_common(50))\n",
    "\n",
    "print('Top 50 # of songs each type occurs in your `all_male` corpus\\n', '='*34, sep='')\n",
    "print(m_song_freq.most_common(50))\n",
    "\n",
    "print('Top 50 bigrams in your `all_male` corpus\\n', '='*34, sep='')\n",
    "print(m_bigrams_dist.most_common(50))\n",
    "\n",
    "print('Top 50 trigrams in your `all_male` corpus\\n', '='*34, sep='')\n",
    "print(m_trigrams_dist.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "Looking at these results, the words \"love\", \"baby\", \"girl\", \"rock\", \"mama\", \"whiskey\", \"world\", and \"beautiful seem to stand out the most to me because they can be used in a variety of contexts. In this `all_male` chart, the bigrams and trigrams list reveal the spread of topics sung about by male artists - be it drinking, heartbreak or love.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Female Artists - 1990s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 words in your `female_90s` corpus\n",
      "==================================\n",
      "[('love', 37), ('oh', 31), ('like', 29), ('let', 29), ('little', 28), ('know', 28), ('yeah', 27), ('boy', 26), ('way', 26), ('go', 23), ('one', 22), ('said', 22), ('knows', 21), ('got', 20), ('baby', 19), ('maybe', 19), ('passionate', 19), ('kisses', 19), ('kiss', 19), ('gonna', 18), ('well', 18), (\"i'm\", 18), ('ever', 18), ('feel', 18), ('right', 17), ('get', 16), ('goodbyes', 16), ('hey', 15), ('think', 15), (\"ain't\", 14), ('man', 14), ('say', 14), ('without', 14), ('back', 13), ('took', 13), ('day', 13), ('make', 13), ('earl', 13), ('away', 12), ('come', 12), ('mama', 12), ('want', 12), ('left', 12), ('prechorus', 12), ('tell', 12), ('ohohoh', 12), ('never', 12), ('long', 11), ('take', 11), ('somewhere', 11)]\n",
      "Top 50 # of songs each type occurs in your `female_90s` corpus\n",
      "==================================\n",
      "[('oh', 11), ('like', 10), ('one', 10), ('said', 10), ('back', 9), ('know', 9), ('got', 8), ('gonna', 8), ('come', 8), (\"ain't\", 8), ('man', 8), ('get', 8), ('love', 8), (\"i'm\", 8), ('good', 8), ('baby', 8), ('go', 8), ('take', 8), ('long', 7), ('away', 7), ('way', 7), ('let', 7), ('night', 7), ('time', 7), ('right', 7), ('boy', 6), ('town', 6), ('left', 6), ('little', 6), ('yeah', 6), ('never', 6), ('still', 5), ('talk', 5), ('mama', 5), ('summer', 5), ('want', 5), ('took', 5), ('life', 5), ('every', 5), ('say', 5), ('well', 5), ('prechorus', 5), ('tell', 5), ('need', 5), ('day', 5), ('around', 5), ('make', 5), (\"let's\", 5), (\"i've\", 5), ('came', 4)]\n",
      "Top 50 bigrams in your `female_90s` corpus\n",
      "==================================\n",
      "[('passionate kisses', 19), ('little goodbyes', 16), ('love boy', 13), ('kisses passionate', 13), ('way love', 12), ('walkaway joe', 11), ('yeah yeah', 9), ('kiss kiss', 9), ('boy love', 8), (\"here's one\", 8), ('one chance', 8), ('chance fancy', 8), ('fancy let', 8), ('hey hey', 8), ('live without', 7), ('watermelon crawl', 7), ('try think', 7), ('independence day', 6), ('feel like', 6), ('strawberry wine', 6), ('heads carolina', 6), ('carolina tails', 6), ('tails california', 6), ('lord knows', 6), (\"said here's\", 5), ('oh oh', 5), ('wide open', 5), ('open spaces', 5), ('knows high', 5), ('high stakes', 5), ('aahhh aahhh', 5), (\"i've got\", 5), ('like strawberry', 5), ('feeling like', 5), ('like centrifugal', 5), ('centrifugal motion', 5), ('hand says', 4), ('go ever', 4), (\"let's go\", 4), ('totally crazy', 4), ('feel way', 4), ('way feel', 4), ('like woman', 4), ('stakes knows', 4), ('maybe memphis', 4), ('memphis maybe', 4), ('maybe southern', 4), ('southern summer', 4), ('summer nights', 4), ('nights maybe', 4)]\n",
      "Top 50 trigrams in your `female_90s` corpus\n",
      "==================================\n",
      "[('passionate kisses passionate', 13), ('kisses passionate kisses', 13), ('love boy love', 8), ('boy love boy', 8), (\"here's one chance\", 8), ('one chance fancy', 8), ('chance fancy let', 8), ('heads carolina tails', 6), ('carolina tails california', 6), (\"said here's one\", 5), ('wide open spaces', 5), ('knows high stakes', 5), ('like strawberry wine', 5), ('feeling like centrifugal', 5), ('like centrifugal motion', 5), ('feel way feel', 4), ('feel like woman', 4), ('aahhh aahhh aahhh', 4), ('yeah yeah yeah', 4), ('maybe memphis maybe', 4), ('memphis maybe southern', 4), ('maybe southern summer', 4), ('southern summer nights', 4), ('summer nights maybe', 4), ('nights maybe maybe', 4), ('maybe maybe sure', 4), ('maybe sure felt', 4), ('sure felt right', 4), ('perpetual bliss pivotal', 4), ('bliss pivotal moment', 4), ('pivotal moment ah', 4), ('way love baby', 4), ('love baby way', 4), ('baby way love', 4), (\"boy's walkaway joe\", 4), ('brains got short', 3), ('got short end', 3), ('short end stick', 3), ('gonna marry boy', 3), ('marry boy someday', 3), ('ever leave baby', 3), ('leave baby would', 3), ('baby would take', 3), ('would take away', 3), ('take away everything', 3), ('life tell live', 3), ('tell live without', 3), ('live without want', 3), ('without want know', 3), ('want know breathe', 3)]\n"
     ]
    }
   ],
   "source": [
    "f_word_freq_90s = Counter() ##tally of # of times each type occurs in TOTAL \n",
    "f_song_freq_90s = Counter() ## tally of # of songs each type occurs in (each type only counted once for each song it occurs in)\n",
    "f_bigrams_dist_90s = Counter()\n",
    "f_trigrams_dist_90s = Counter()\n",
    "\n",
    "# 1. loop over each song in chart\n",
    "for song in all_charts['female_90s']:\n",
    "    raw_lyrics = song['lyrics']\n",
    "    song_toks = []\n",
    "    temp = tokenize(raw_lyrics, lowercase=True,strip_chars=char_to_strip)\n",
    "    for tok in temp:\n",
    "        if tok not in stop_words:\n",
    "            song_toks.append(tok)\n",
    "    f_word_freq_90s.update(song_toks)\n",
    "    unique_type = set(song_toks) \n",
    "    f_song_freq_90s.update(unique_type) # update song_freq_90s with the types (i.e. unique values in tokens)\n",
    "    f_bigrams_90s = get_ngram_tokens(song_toks, n=2)\n",
    "    f_bigrams_dist_90s.update(f_bigrams_90s)\n",
    "    f_trigrams_90s = get_ngram_tokens(song_toks, n=3)\n",
    "    f_trigrams_dist_90s.update(f_trigrams_90s)\n",
    "\n",
    "print('Top 50 words in your `female_90s` corpus\\n', '='*34, sep='')\n",
    "print(f_word_freq_90s.most_common(50))\n",
    "\n",
    "print('Top 50 # of songs each type occurs in your `female_90s` corpus\\n', '='*34, sep='')\n",
    "print(f_song_freq_90s.most_common(50))\n",
    "\n",
    "print('Top 50 bigrams in your `female_90s` corpus\\n', '='*34, sep='')\n",
    "print(f_bigrams_dist_90s.most_common(50))\n",
    "\n",
    "print('Top 50 trigrams in your `female_90s` corpus\\n', '='*34, sep='')\n",
    "print(f_trigrams_dist_90s.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "Looking at the songs written by female artists in the 1990s (`female_90s`), again there is a theme of \"love\" and love-related words (\"love\", \"passionate\", \"kisses\"). An interesting distinction in this data set is, apart from the allusions to love, there is a theme about seasons and geographies (\"carolina\", \"california\", \"summer\", \"memphis\").  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Male Artists - 1990s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 words in your `male_90s` corpus\n",
      "==================================\n",
      "[('love', 71), (\"i'm\", 57), ('know', 46), ('oh', 39), ('wanna', 34), ('ya', 34), ('girl', 31), ('get', 31), ('got', 30), ('yeah', 29), (\"ain't\", 28), ('maria', 27), ('never', 26), ('like', 26), ('heart', 25), (\"i've\", 25), ('tell', 24), ('take', 23), ('come', 21), ('say', 19), ('make', 19), ('let', 18), ('huh', 18), ('away', 17), ('little', 17), (\"can't\", 17), ('would', 16), ('one', 16), ('time', 16), (\"i'll\", 16), ('beautiful', 16), ('ay', 16), ('world', 15), ('start', 15), ('night', 14), ('done', 14), ('hurry', 14), ('rush', 14), (\"slippin'\", 14), ('see', 13), ('think', 13), ('gotta', 13), ('dance', 13), ('want', 12), ('marie', 12), ('keep', 12), ('man', 11), ('way', 11), ('friends', 11), ('baby', 11)]\n",
      "Top 50 # of songs each type occurs in your `male_90s` corpus\n",
      "==================================\n",
      "[('know', 13), (\"i'm\", 13), ('night', 12), ('never', 11), ('love', 11), ('like', 10), (\"i've\", 10), ('got', 10), ('oh', 8), ('one', 8), ('girl', 8), ('right', 8), ('get', 8), ('said', 7), ('heart', 7), ('let', 7), ('time', 7), (\"can't\", 7), ('eyes', 7), (\"ain't\", 7), ('go', 6), ('say', 6), ('man', 6), ('yeah', 6), ('long', 6), ('world', 6), ('sweet', 6), ('take', 6), ('way', 6), ('come', 6), ('away', 5), ('thought', 5), ('make', 5), ('last', 5), (\"'cause\", 5), ('says', 5), ('mind', 5), ('little', 5), ('see', 5), ('feel', 5), ('keep', 5), ('baby', 5), ('ever', 4), ('heard', 4), (\"i'd\", 4), ('place', 4), (\"holdin'\", 4), ('show', 4), (\"there's\", 4), ('gonna', 4)]\n",
      "Top 50 bigrams in your `male_90s` corpus\n",
      "==================================\n",
      "[('ay yeah', 16), (\"i'm hurry\", 14), ('love start', 14), (\"start slippin'\", 14), ('would ya', 14), ('know beautiful', 13), ('maria oh', 12), ('oh marie', 12), ('marie love', 12), ('love girl', 12), ('got friends', 9), ('friends low', 9), ('low places', 9), ('tim mcgraw', 9), ('yeah ay', 9), ('tell heart', 8), ('heart achy', 8), ('achy breaky', 8), ('breaky heart', 8), ('like love', 8), ('say wanna', 8), ('wanna get', 8), (\"i've got\", 7), ('chasing neon', 7), ('neon rainbow', 7), ('living honkytonk', 7), ('honkytonk dream', 7), ('hurry get', 7), ('get things', 7), ('things done', 7), ('rush rush', 7), (\"rush life's\", 7), (\"life's fun\", 7), ('fun really', 7), ('really gotta', 7), ('gotta live', 7), ('live die', 7), (\"die i'm\", 7), ('hurry know', 7), ('let love', 7), (\"slippin' love\", 7), (\"slippin' away\", 7), ('could ya', 7), ('ya would', 7), (\"ya ain't\", 7), (\"ain't ya\", 7), ('ya gonna', 7), ('gonna asked', 7), ('asked would', 7), ('ya wanna', 7)]\n",
      "Top 50 trigrams in your `male_90s` corpus\n",
      "==================================\n",
      "[(\"love start slippin'\", 14), ('maria oh marie', 12), ('oh marie love', 12), ('marie love girl', 12), ('got friends low', 9), ('friends low places', 9), ('ay yeah ay', 9), ('yeah ay yeah', 9), ('tell heart achy', 8), ('heart achy breaky', 8), ('achy breaky heart', 8), ('chasing neon rainbow', 7), ('living honkytonk dream', 7), (\"i'm hurry get\", 7), ('hurry get things', 7), ('get things done', 7), (\"rush rush life's\", 7), (\"rush life's fun\", 7), (\"life's fun really\", 7), ('fun really gotta', 7), ('really gotta live', 7), ('gotta live die', 7), (\"live die i'm\", 7), (\"die i'm hurry\", 7), (\"i'm hurry know\", 7), ('let love start', 7), (\"start slippin' love\", 7), (\"slippin' love start\", 7), (\"start slippin' away\", 7), ('could ya would', 7), ('ya would ya', 7), (\"would ya ain't\", 7), (\"ya ain't ya\", 7), (\"ain't ya gonna\", 7), ('ya gonna asked', 7), ('gonna asked would', 7), ('asked would ya', 7), ('would ya wanna', 7), ('ya wanna baby', 7), ('wanna baby tonight', 7), ('tim mcgraw &', 7), ('mcgraw & faith', 7), ('& faith hill', 7), ('whenever come around', 7), ('say wanna get', 7), ('love girl oh', 6), ('girl oh maria', 6), ('oh maria maria', 6), ('maria maria oh', 6), ('love girl maria', 6)]\n"
     ]
    }
   ],
   "source": [
    "m_word_freq_90s = Counter() ##tally of # of times each type occurs in TOTAL \n",
    "m_song_freq_90s = Counter() ## tally of # of songs each type occurs in (each type only counted once for each song it occurs in)\n",
    "m_bigrams_dist_90s = Counter()\n",
    "m_trigrams_dist_90s = Counter()\n",
    "\n",
    "# 1. loop over each song in chart\n",
    "for song in all_charts['male_90s']:\n",
    "    raw_lyrics = song['lyrics']\n",
    "    song_toks = []\n",
    "    temp = tokenize(raw_lyrics, lowercase=True,strip_chars=char_to_strip)\n",
    "    for tok in temp:\n",
    "        if tok not in stop_words:\n",
    "            song_toks.append(tok)\n",
    "    m_word_freq_90s.update(song_toks)\n",
    "    unique_type = set(song_toks) \n",
    "    m_song_freq_90s.update(unique_type) \n",
    "    m_bigrams_90s = get_ngram_tokens(song_toks, n=2)\n",
    "    m_bigrams_dist_90s.update(m_bigrams_90s)\n",
    "    m_trigrams_90s = get_ngram_tokens(song_toks, n=3)\n",
    "    m_trigrams_dist_90s.update(m_trigrams_90s)\n",
    "\n",
    "print('Top 50 words in your `male_90s` corpus\\n', '='*34, sep='')\n",
    "print(m_word_freq_90s.most_common(50))\n",
    "\n",
    "print('Top 50 # of songs each type occurs in your `male_90s` corpus\\n', '='*34, sep='')\n",
    "print(m_song_freq_90s.most_common(50))\n",
    "\n",
    "print('Top 50 bigrams in your `male_90s` corpus\\n', '='*34, sep='')\n",
    "print(m_bigrams_dist_90s.most_common(50))\n",
    "\n",
    "print('Top 50 trigrams in your `male_90s` corpus\\n', '='*34, sep='')\n",
    "print(m_trigrams_dist_90s.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "Given the theme of \"love\" that I've been picking up on from earlier analysis of the list of country songs, it is interesting that '90s male artists mention the word \"love\" 71 times while their female counterparts from the '90s mention \"love\" only 37 times; male artists mention \"love\" nearly twice as much! I am excited to explore this disparity more throughout this project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Female Artists - 2010s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 words in your `female_2010s` corpus\n",
      "==================================\n",
      "[('yeah', 56), ('every', 40), (\"i'm\", 40), ('back', 36), ('like', 36), ('get', 36), ('little', 36), ('hope', 33), ('never', 32), ('got', 31), ('oh', 30), ('away', 26), ('take', 26), (\"ain't\", 26), ('changed', 25), ('know', 22), ('could', 21), ('name', 21), ('gonna', 20), ('thing', 20), ('heart', 19), ('ever', 19), ('road', 18), ('one', 17), ('blown', 16), ('stronger', 16), ('good', 15), ('time', 15), (\"i'll\", 15), ('traveled', 15), ('let', 14), ('place', 14), ('see', 14), ('put', 14), ('along', 14), ('way', 14), ('less', 14), ('house', 13), ('remember', 13), ('water', 13), ('around', 13), ('make', 13), (\"'em\", 13), ('girl', 13), ('said', 12), ('prechorus', 12), ('love', 12), (\"i've\", 12), ('go', 12), ('find', 12)]\n",
      "Top 50 # of songs each type occurs in your `female_2010s` corpus\n",
      "==================================\n",
      "[('like', 11), ('get', 11), ('yeah', 10), ('back', 10), ('one', 10), ('heart', 9), ('good', 8), ('got', 8), (\"i'm\", 8), ('time', 8), ('little', 8), ('know', 7), ('every', 7), ('ever', 7), ('around', 7), ('put', 7), ('look', 7), (\"i'll\", 7), ('take', 7), ('along', 7), (\"ain't\", 7), ('oh', 7), ('gonna', 6), ('prechorus', 6), ('said', 6), ('place', 6), ('never', 6), (\"'cause\", 6), ('made', 6), ('see', 6), ('make', 6), ('say', 6), ('world', 6), ('right', 6), ('go', 6), ('away', 5), ('smile', 5), ('keep', 5), ('thought', 5), ('daddy', 5), ('mine', 5), ('turn', 5), ('last', 5), ('could', 5), (\"i've\", 5), ('love', 5), ('day', 5), ('days', 5), (\"i'd\", 5), ('baby', 5)]\n",
      "Top 50 bigrams in your `female_2010s` corpus\n",
      "==================================\n",
      "[('got name', 19), ('name changed', 18), ('changed back', 18), ('blown away', 16), ('yeah yeah', 15), ('little thing', 15), ('take road', 14), ('road less', 14), ('less traveled', 14), ('every little', 14), ('back yeah', 12), ('little bit', 11), (\"let 'em\", 9), ('afraid take', 9), ('bit stronger', 9), (\"i'm done\", 9), ('never gonna', 8), ('oh oohwhoa', 8), ('something water', 8), ('away blown', 7), ('burning house', 7), (\"like i'm\", 7), ('get little', 7), ('miranda lambert', 7), ('black like', 7), ('lost boy', 6), ('nothing left', 6), (\"i've sleepwalking\", 6), ('yeah guess', 6), ('guess that’s', 6), ('that’s church', 6), ('kiss lots', 6), ('follow arrow', 6), ('arrow wherever', 6), ('wherever points', 6), ('girl country', 6), ('country song', 6), ('must something', 6), ('hope cheats', 6), ('cheats like', 6), ('remember every', 6), ('fly away', 5), ('gonna man', 5), (\"careless man's\", 5), (\"man's careful\", 5), ('careful daughter', 5), ('daughter best', 5), ('best thing', 5), (\"thing that's\", 5), (\"that's ever\", 5)]\n",
      "Top 50 trigrams in your `female_2010s` corpus\n",
      "==================================\n",
      "[('got name changed', 18), ('name changed back', 18), ('take road less', 14), ('road less traveled', 14), ('every little thing', 14), ('changed back yeah', 12), ('back yeah yeah', 12), ('afraid take road', 9), ('little bit stronger', 9), ('blown away blown', 7), ('away blown away', 7), ('yeah guess that’s', 6), ('guess that’s church', 6), ('follow arrow wherever', 6), ('arrow wherever points', 6), ('girl country song', 6), ('must something water', 6), ('hope cheats like', 6), ('remember every little', 6), ('never gonna man', 5), (\"careless man's careful\", 5), (\"man's careful daughter\", 5), ('careful daughter best', 5), ('daughter best thing', 5), (\"best thing that's\", 5), (\"thing that's ever\", 5), (\"that's ever mine\", 5), ('less traveled wear', 5), ('traveled wear boots', 5), ('wear boots kick', 5), ('boots kick gravel', 5), ('kick gravel afraid', 5), ('gravel afraid take', 5), ('yeah yeah got', 5), ('yeah got name', 5), ('changed back got', 5), ('back got name', 5), ('little thing remember', 5), ('thing remember every', 5), ('little thing high', 5), ('thing high hurt', 5), ('high hurt shine', 5), ('hurt shine sting', 5), ('shine sting every', 5), ('sting every little', 5), ('made rebel careless', 4), (\"rebel careless man's\", 4), ('get hallelujah get', 4), ('hallelujah get amen', 4), ('get amen feels', 4)]\n"
     ]
    }
   ],
   "source": [
    "f_word_freq_2010s = Counter() ##tally of # of times each type occurs in TOTAL \n",
    "f_song_freq_2010s = Counter() ## tally of # of songs each type occurs in (each type only counted once for each song it occurs in)\n",
    "f_bigrams_dist_2010s = Counter()\n",
    "f_trigrams_dist_2010s = Counter()\n",
    "\n",
    "# 1. loop over each song in chart\n",
    "for song in all_charts['female_2010s']:\n",
    "    raw_lyrics = song['lyrics']\n",
    "    song_toks = []\n",
    "    temp = tokenize(raw_lyrics, lowercase=True,strip_chars=char_to_strip)\n",
    "    for tok in temp:\n",
    "        if tok not in stop_words:\n",
    "            song_toks.append(tok)\n",
    "    f_word_freq_2010s.update(song_toks)\n",
    "    unique_type = set(song_toks) \n",
    "    f_song_freq_2010s.update(unique_type) \n",
    "    f_bigrams_2010s = get_ngram_tokens(song_toks, n=2)\n",
    "    f_bigrams_dist_2010s.update(f_bigrams_2010s)\n",
    "    f_trigrams_2010s = get_ngram_tokens(song_toks, n=3)\n",
    "    f_trigrams_dist_2010s.update(f_trigrams_2010s)\n",
    "\n",
    "print('Top 50 words in your `female_2010s` corpus\\n', '='*34, sep='')\n",
    "print(f_word_freq_2010s.most_common(50))\n",
    "\n",
    "print('Top 50 # of songs each type occurs in your `female_2010s` corpus\\n', '='*34, sep='')\n",
    "print(f_song_freq_2010s.most_common(50))\n",
    "\n",
    "print('Top 50 bigrams in your `female_2010s` corpus\\n', '='*34, sep='')\n",
    "print(f_bigrams_dist_2010s.most_common(50))\n",
    "\n",
    "print('Top 50 trigrams in your `female_2010s` corpus\\n', '='*34, sep='')\n",
    "print(f_trigrams_dist_2010s.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "Looking at the most common tokens, there does not seem to be any obvious connections between any of the words that could relate to a bigger theme through songs by female artists of the 2010. Keeping this in mind, songs written by '90s female artists contained lyrics relating to \"love\"; I am interested to see if there are any hidden messages/themes that will be unveiled in my other analyses notebooks with this specific corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Male Artists - 2010s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 words in your `male_2010s` corpus\n",
      "==================================\n",
      "[('like', 72), (\"i'm\", 57), ('back', 44), ('baby', 40), ('oooh', 36), ('yeah', 31), ('need', 28), ('rock', 28), ('got', 27), ('right', 27), (\"ain't\", 27), ('little', 26), (\"'em\", 26), ('mama', 26), ('make', 25), ('think', 25), ('know', 24), ('get', 24), ('gonna', 23), ('go', 23), ('wanna', 23), ('see', 21), ('hey', 20), ('dirt', 19), ('one', 19), ('good', 18), ('way', 17), ('whiskey', 17), ('used', 16), ('road', 16), (\"i'ma\", 16), ('tequila', 16), ('around', 15), ('night', 15), ('hell', 15), ('free', 15), ('drink', 14), ('take', 14), (\"'cause\", 14), ('man', 14), ('ya', 14), ('crazy', 14), ('always', 14), ('country', 13), ('never', 13), ('glasses', 13), ('drunk', 13), (\"can't\", 12), ('feel', 12), ('shine', 12)]\n",
      "Top 50 # of songs each type occurs in your `male_2010s` corpus\n",
      "==================================\n",
      "[('like', 12), ('go', 12), ('back', 12), (\"i'm\", 11), ('yeah', 11), ('know', 11), ('get', 11), (\"ain't\", 10), ('got', 9), ('baby', 9), ('right', 8), ('take', 8), ('way', 8), ('feel', 7), ('see', 7), ('night', 7), ('need', 7), ('new', 7), ('make', 6), ('good', 6), (\"can't\", 6), (\"we're\", 6), ('little', 6), ('think', 6), ('old', 6), ('hell', 6), ('sun', 6), ('come', 6), ('oh', 6), ('never', 6), ('long', 5), ('drink', 5), ('home', 5), ('lights', 5), ('around', 5), ('one', 5), ('town', 5), ('time', 5), ('road', 5), ('man', 5), ('getting', 5), ('help', 5), ('gonna', 4), ('really', 4), ('took', 4), ('watch', 4), ('funny', 4), ('times', 4), (\"i'll\", 4), ('day', 4)]\n",
      "Top 50 bigrams in your `male_2010s` corpus\n",
      "==================================\n",
      "[('oooh oooh', 34), ('rock mama', 18), ('whiskey glasses', 13), ('make wanna', 12), ('mama like', 12), ('little dirt', 11), ('dirt boots', 11), (\"god's country\", 11), ('goes like', 11), ('taste tequila', 11), ('always stay', 10), ('stay humble', 10), ('humble kind', 10), ('mama rock', 10), ('broken halos', 10), ('got little', 9), ('roll windows', 9), ('windows cruise', 9), (\"line 'em\", 9), (\"knock 'em\", 9), (\"i'ma need\", 9), ('drunk plane', 9), (\"i'm gonna\", 8), ('might little', 8), ('baby song', 8), ('song make', 8), ('wanna roll', 8), ('back road', 8), ('like back', 8), (\"'em back\", 8), (\"fill 'em\", 8), ('hey mama', 8), ('think think', 7), ('see world', 7), ('world whiskey', 7), (\"i'm getting\", 7), ('getting drunk', 7), ('halos used', 7), ('used shine', 7), ('dirt road', 6), (\"that's right\", 6), ('free free', 6), (\"free we'll\", 6), (\"we'll ever\", 6), ('woahohohoh woahohohoh', 6), ('woahohohoh woahohohohohohoh', 6), ('looks good', 6), ('need whiskey', 6), (\"'em line\", 6), ('back knock', 6)]\n",
      "Top 50 trigrams in your `male_2010s` corpus\n",
      "==================================\n",
      "[('oooh oooh oooh', 32), ('rock mama like', 12), ('little dirt boots', 11), ('always stay humble', 10), ('stay humble kind', 10), ('baby song make', 8), ('song make wanna', 8), ('make wanna roll', 8), ('wanna roll windows', 8), ('roll windows cruise', 8), (\"knock 'em back\", 8), ('hey mama rock', 8), ('see world whiskey', 7), ('world whiskey glasses', 7), (\"i'm getting drunk\", 7), ('getting drunk plane', 7), ('broken halos used', 7), ('halos used shine', 7), ('got little dirt', 6), (\"free free we'll\", 6), (\"free we'll ever\", 6), ('woahohohoh woahohohoh woahohohohohohoh', 6), (\"i'ma need whiskey\", 6), ('need whiskey glasses', 6), (\"line 'em line\", 6), (\"'em line 'em\", 6), (\"'em back knock\", 6), (\"back knock 'em\", 6), (\"fill 'em fill\", 6), (\"'em fill 'em\", 6), ('might little dirt', 5), ('goes like hey', 5), ('die go hell', 5), ('mama rock rock', 5), ('saw light sunrise', 4), (\"light sunrise sittin'\", 4), (\"sunrise sittin' back\", 4), (\"sittin' back 40\", 4), ('back 40 muddy', 4), ('40 muddy riverside', 4), (\"muddy riverside gettin'\", 4), (\"riverside gettin' baptized\", 4), (\"gettin' baptized holy\", 4), ('baptized holy water', 4), (\"holy water 'shine\", 4), (\"water 'shine dogs\", 4), (\"'shine dogs runnin'\", 4), ('saved sound found', 4), ('sound found dixie', 4), ('found dixie whistled', 4)]\n"
     ]
    }
   ],
   "source": [
    "m_word_freq_2010s = Counter() ##tally of # of times each type occurs in TOTAL \n",
    "m_song_freq_2010s = Counter() ## tally of # of songs each type occurs in (each type only counted once for each song it occurs in)\n",
    "m_bigrams_dist_2010s = Counter()\n",
    "m_trigrams_dist_2010s = Counter()\n",
    "\n",
    "# 1. loop over each song in chart\n",
    "for song in all_charts['male_2010s']:\n",
    "    raw_lyrics = song['lyrics']\n",
    "    song_toks = []\n",
    "    temp = tokenize(raw_lyrics, lowercase=True,strip_chars=char_to_strip)\n",
    "    for tok in temp:\n",
    "        if tok not in stop_words:\n",
    "            song_toks.append(tok)\n",
    "    m_word_freq_2010s.update(song_toks)\n",
    "    unique_type = set(song_toks) \n",
    "    m_song_freq_2010s.update(unique_type) \n",
    "    m_bigrams_2010s = get_ngram_tokens(song_toks, n=2)\n",
    "    m_bigrams_dist_2010s.update(m_bigrams_2010s)\n",
    "    m_trigrams_2010s = get_ngram_tokens(song_toks, n=3)\n",
    "    m_trigrams_dist_2010s.update(m_trigrams_2010s)\n",
    "\n",
    "print('Top 50 words in your `male_2010s` corpus\\n', '='*34, sep='')\n",
    "print(m_word_freq_2010s.most_common(50))\n",
    "\n",
    "print('Top 50 # of songs each type occurs in your `male_2010s` corpus\\n', '='*34, sep='')\n",
    "print(m_song_freq_2010s.most_common(50))\n",
    "\n",
    "print('Top 50 bigrams in your `male_2010s` corpus\\n', '='*34, sep='')\n",
    "print(m_bigrams_dist_2010s.most_common(50))\n",
    "\n",
    "print('Top 50 trigrams in your `male_2010s` corpus\\n', '='*34, sep='')\n",
    "print(m_trigrams_dist_2010s.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "Unlike the `female_2010s` data, the `male_2010s` chart reveals some possible themes with words such as \"baby\", \"mama\", \"road\", \"whiskey\", \"tequila\", and \"drunk\". Again, with the bigrams and trigrams, I can pick up on what songs such word pairs belong to.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word       1990s      2010s     \n",
      "==============================\n",
      "beer       5          9         \n",
      "boots      1          17        \n",
      "drink      3          16        \n",
      "drinking   0          0         \n",
      "drinks     0          6         \n",
      "truck      9          5         \n",
      "road       5          34        \n",
      "yeah       56         87        \n",
      "baby       30         52        \n",
      "girl       35         22        \n",
      "love       108        19        \n",
      "little     45         62        \n",
      "drunk      0          14        \n",
      "whiskey    6          20        \n"
     ]
    }
   ],
   "source": [
    "## comparing some key words present in BOTH the 90s and 2010s lyrics - key words found on various websites\n",
    "\n",
    "words_to_compare = ['beer','boots','drink','drinking','drinks','truck','road','yeah','baby','girl','love','little','drunk','whiskey']\n",
    "print(\"{:<10} {:<10} {:<10}\".format(\"word\", \"1990s\", \"2010s\"))\n",
    "print(\"=\"*30)\n",
    "\n",
    "for word in words_to_compare:\n",
    "    print(\"{:<10} {:<10} {:<10}\".format(word,\n",
    "                                   word_freq_90s.get(word,0),\n",
    "                                   word_freq_2010s.get(word,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the chart above, it is clear that songs from the 2010s referenced alcohol and drinking significantly more than songs from the 1990s. The most significant difference that stands out to me is how the word \"love\" in the '90s was used almost six times more than it was used in the 2010s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word       1990s      2010s     \n",
      "==============================\n",
      "heart      30         27        \n",
      "love       108        19        \n",
      "baby       30         52        \n",
      "girl       35         22        \n",
      "yeah       56         87        \n",
      "man        25         21        \n",
      "little     45         62        \n"
     ]
    }
   ],
   "source": [
    "## Comparing some key words present in BOTH the 90s and 2010s lyrics\n",
    "## Key words chosen from printed results earlier in this notebook \n",
    "\n",
    "words_to_compare = ['heart','love','baby','girl','yeah','man','little']\n",
    "print(\"{:<10} {:<10} {:<10}\".format(\"word\", \"1990s\", \"2010s\"))\n",
    "print(\"=\"*30)\n",
    "\n",
    "for word in words_to_compare:\n",
    "    print(\"{:<10} {:<10} {:<10}\".format(word,\n",
    "                                   word_freq_90s.get(word,0),\n",
    "                                   word_freq_2010s.get(word,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After putting words that appeared in both corpora into this chart, I found it easier to make comparisons. The words \"heart\", \"man\", \"little\", and \"girl\" were used almost equally, with \"baby\", \"yeah\" and \"love\" being the words with the biggest disparities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Words Proportions between Different Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love 108 6.81\n",
      "heart 30 1.89\n",
      "happy 1 0.06\n",
      "romance 4 0.25\n",
      "lover 1 0.06\n",
      "smile 11 0.69\n",
      "baby 30 1.89\n"
     ]
    }
   ],
   "source": [
    "love_rel_words = ['love','heart','happy','romance','lover','smile','baby']\n",
    "\n",
    "for word in love_rel_words:\n",
    "    lovecnt_proportion90s = word_freq_90s[word]/len(word_freq_90s)*100\n",
    "    lovecnt_percentage90s = round(lovecnt_proportion90s, 2)\n",
    "    print(word, word_freq_90s[word],lovecnt_percentage90s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love 19 1.36\n",
      "heart 27 1.94\n",
      "happy 3 0.22\n",
      "romance 0 0.0\n",
      "lover 0 0.0\n",
      "smile 11 0.79\n",
      "baby 52 3.73\n"
     ]
    }
   ],
   "source": [
    "love_rel_words = ['love','heart','happy','romance','lover','smile','baby']\n",
    "\n",
    "for word in love_rel_words:\n",
    "    lovecnt_proportion2010s = word_freq_2010s[word]/len(word_freq_2010s)*100\n",
    "    lovecnt_percentage2010s = round(lovecnt_proportion2010s, 2)\n",
    "    print(word, word_freq_2010s[word],lovecnt_percentage2010s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations: \n",
    "\n",
    "- Keeping in mind that these percentages account for all the filler words of \"i'm\", \"get\", \"every\", etc., \"love\" is used quite frequently (as noted multiple times in this notebook), with \"heart\" and \"baby\" being used the most next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whiskey 20 1.43\n",
      "beer 9 0.65\n",
      "drunk 14 1.0\n",
      "tequila 16 1.15\n",
      "drink 16 1.15\n"
     ]
    }
   ],
   "source": [
    "drinking_rel_words = ['whiskey','beer','drunk','tequila','drink']\n",
    "\n",
    "for word in drinking_rel_words:\n",
    "    drinkcnt_proportion2010s = word_freq_2010s[word]/len(word_freq_2010s)*100\n",
    "    drinkcnt_percentage2010s = round(drinkcnt_proportion2010s, 2)\n",
    "    print(word, word_freq_2010s[word],drinkcnt_percentage2010s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whiskey 6 0.38\n",
      "beer 5 0.32\n",
      "drunk 0 0.0\n",
      "tequila 0 0.0\n",
      "drink 3 0.19\n"
     ]
    }
   ],
   "source": [
    "drinking_rel_words = ['whiskey','beer','drunk','tequila','drink']\n",
    "\n",
    "for word in drinking_rel_words:\n",
    "    drinkcnt_proportion90s = word_freq_90s[word]/len(word_freq_90s)*100\n",
    "    drinkcnt_percentage90s = round(drinkcnt_proportion90s, 2)\n",
    "    print(word, word_freq_90s[word],drinkcnt_percentage90s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "\n",
    "- Not much reference to drinking in either corpora\n",
    "- Drink references uses more in 2010s lyrics than 1990s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
